---
title: "Prior Distributions for Tau and PI"
output: html_document
---

```{r message=FALSE, warning=FALSE}
#library(httr)
#library(jsonlite)
library(zoo)
library(dplyr)
library(ggplot2)
library(fitdistrplus)
library(ggpubr)
```

```{r}
path.out.figures <- "1_Figures" # Change to where to write output files to
```

# Network Transmission- Tau 

We want to estimate a prior distribution for the Secondary attack (infection) rate: https://www.cdc.gov/csels/dsepd/ss1978/lesson3/section2.html

We have two datasets- provided by Lewis et al. 202 and Jing et al. 2020 - that we can use to estimate a prior distribtion for tau. 
```{r}
path.in <- "5_SAR datasets" # Specify folder locatin of inputs
Lewis2020 <- read.csv(file.path(path.in, "Lewis2020.csv"))
Jing2020 <- read.csv(file.path(path.in, "Jing2020.csv"))
```

## Lewis et al 2020: 
- 58 households
- 32 households had evidence of secondary infection 
- SIR was 28\% (n=52/188) 
- Study of household transmission. Study was conducted March - May 2020 (period in which shelter-in-place orders and social distancing recommendations were largely in place across the US) in Utah and Wisconsin. These states chosen due to relatively low COVID-19 prevalence in order to reduce the risk of additional community exposure to household contacts. 

```{r}
# calculate SIR for entire study population 
sum(Lewis2020$any_pos)

table(Lewis2020$member_type) # member_type =1 is index case; = 0 is household contact

new_cases <- sum(Lewis2020$any_pos)- sum(Lewis2020$member_type)
new_cases / (nrow(Lewis2020) - sum(Lewis2020$member_type)) 
# 58 primary infections
# 52 secondary infections (numerator)
# 188 contacts (denominator)
# SIR = 28% (same as found in Lewis 2020 paper)
# SAR = number new cases among contacts / total number of contacts
```

Note: 
`new_hh_memnum` gives the total number of people in the household
However, 9 individuals declined to participate
`house_mem_count` gives the number of people in the household who participated
We will estimate SAR assuming the total size of the household is given by house_mem_count
```{r}
# create unique householdid var
Lewis2020$household_id <- gsub("(\\d+)[^-]*$", "", Lewis2020$study_id_merge)
Lewis2020$household_id <- gsub("-", "", Lewis2020$household_id)
# https://stackoverflow.com/questions/23641879/regular-expression-match-all-numbers-after-the-last-dash

```


Estimate a secondary infection rate for each household/cluster in the data set and then fit a distribution to these taus
```{r}
Lewis_SAR <- Lewis2020 %>% 
  group_by(household_id) %>%
  summarise(household_id = household_id,
            secondary = sum(any_pos)-1,
            num_contacts = housemem_count-1,
            SAR = secondary/ num_contacts, 
            data_id = "Lewis2020") %>%
            distinct()

```

## Jing et al 2020
```{r}
# for now, lets just look at household secondary attack rate
Jing2020_house <- Jing2020 %>% filter(Same.Residential.Address == 1)
Jing2020_house$any_pos <- ifelse(Jing2020_house$Case.Type == "primary case" | Jing2020_house$Case.Type == "secondary case", 
                                 1, 0)
# 14 times where there is more than one primary case in a cluster
Jing2020_house %>% count(Cluster.ID, Case.Type) %>% summarise(bad = sum(ifelse(Case.Type == "primary case" & n > 1, 1, 0)))
# cluster.id where there is more than one primary case
remove_clusters <- Jing2020_house %>% count(Cluster.ID, Case.Type) %>% filter(Case.Type == "primary case" & n > 1) %>% dplyr::select(Cluster.ID)

Jing2020_house2 <- subset(Jing2020_house, !Jing2020_house$Cluster.ID %in% remove_clusters$Cluster.ID)
Jing2020_house2$IsSecondary <- ifelse(Jing2020_house2$Case.Type == "secondary case", 1, 0)

temp <- Jing2020_house2 %>% count(Cluster.ID)
table(temp$n) # 19 clusters that only contain 1 person (primary case)
# these 19 clusters are dropped (can't infer valid SAR)
length(unique(Jing2020_house2$Cluster.ID)) # 168 clusters total

Jing_SAR <- Jing2020_house2 %>% 
  filter(Case.Type == "secondary case" | Case.Type == "contact") %>%
  group_by(Cluster.ID) %>%
  summarise(household_id = Cluster.ID,
            secondary = sum(IsSecondary), 
            num_contacts = Size..Same.Residential.Address.-1, 
            SAR = secondary/num_contacts, 
            data_id = "Jing2020") %>%
  distinct() # 149 clusters after dropping the 19 that only have 1 person
mean(Jing_SAR$SAR)
# SAR = number new cases among contacts / total number of contacts
```

## Fit distribution to data 
Merge into one SAR dataset
```{r}
# keep columns that are same in both data sets
common_col <- c(intersect(names(Jing_SAR), names(Lewis_SAR)))
all_SAR <- rbind(Jing_SAR[,common_col], Lewis_SAR[,common_col])
```




### EM Algorithm
1. Start with uniform prior on $\tau$: Beta(alpha=1, beta=1)

2. E step: impute $tau_g$ *for each cluster* using the mean of the beta posterior: 
$$
\tau_g = \frac{\alpha + s_g}{\alpha + \beta + N_g}
$$
where $s_g$ is the number of secondary cases and $N_g$ is the number of contacts in group $g$. 

3. M step: impute $\alpha$ and $\beta$ using the method of moment estimates for a Beta distribution (see here: https://en.wikipedia.org/wiki/Beta_distribution#Parameter_estimation)

4. Repeat the E and M steps, updating $\alpha$ and $\beta$ each time until the parameter values converge, i.e. stabilize such that
$$
(\alpha_{old} - \alpha_{new})^2 + (\beta_{old} - \beta_{new})^2 < \epsilon
$$
```{r}
hist(all_SAR$SAR, breaks = 30)

hist(all_SAR$secondary, breaks=30)

hist(all_SAR$num_contacts, breaks=30)

taudat_hist1 <- ggplot(all_SAR, aes(x=num_contacts)) + 
  geom_histogram() + ggtitle("Number of Contacts") + theme_bw()

tau_dat_hist2 <- ggplot(all_SAR, aes(x=secondary)) + 
  geom_histogram() + ggtitle("Number Secondary Infections") + theme_bw()

pdf(file.path(path.out.figures, "taudat_hists.pdf"), width =8, height =5)
ggarrange(taudat_hist1, tau_dat_hist2)
dev.off()
```
```{r}
sum(all_SAR$secondary)/sum(all_SAR$num_contacts)
```


```{r}
b <- 10
# initialize parameters
alpha <- numeric(b)
beta <- numeric(b)
m1 <- numeric(b)
m2 <- numeric(b)
alpha[1] <- 1
beta[1] <- 1
N <- nrow(all_SAR)

for (i in 2:b){
  # E step
  tau_g <- (alpha[i-1] + all_SAR$secondary)/(alpha[i-1] + beta[i-1] + all_SAR$num_contacts)
  print(paste("tau_g: ", round(head(tau_g), 2)))
  
  # M step
  m1[i] <- mean(tau_g)
  m2[i] <- mean(tau_g^2)
  print(paste("m1= ", round(m1[i], 3), "m2= ", round(m2[i], 3)))
  mfrac <- m2[i]/m1[i]
  #denom <- (all_SAR$num_contacts*(mfrac - m1[i] - 1) + m1[i])
  denom <- (n*(mfrac - m1[i] - 1) + m1[i])
  
  alpha[i] <- (n*m1[i] - m2[i]) / denom
  beta[i] <- ((n - m1[i]*(n - mfrac))) / denom  
  #alpha[i] <- (all_SAR$num_contacts*m1[i] - m2[i]) / denom
  #beta[i] <- ((all_SAR$num_contacts - m1[i]*(all_SAR$num_contacts - mfrac))) / denom
  print(paste("alpha= ", alpha[i], "beta= ", beta[i]))
  print((alpha[i-1]-alpha[i])^2 + (beta[i-1]-beta[i])^2)
}
```

## Beta Binomial Distribution
EM algorithm doesn't converge- probably because data is zero inflated
Rather than using EM, we can reparameterize the problem and then have closed form solutions

Use this method: 
https://en.wikipedia.org/wiki/Beta-binomial_distribution#Further_Bayesian_considerations

We reparameterize the distrbiution so that the expected mean of the prior is a single parameter: 
$$
Beta(M \mu, M(1-\mu))
$$
where
$$
\mu = \frac{\alpha}{\alpha + \beta} \\

M = \alpha + \beta
$$

Let $k_i$ be the number of successes out of $n_i$ trials for event $i$: 
$$
k_i \sim Bin(n_i, \theta_i) \\
\theta_i \sim Beta(\mu, M), i.i.d
$$

To find the point estimates of $\mu$ and $M$ we use the sample: 
$$
\hat{\mu} = \frac{\sum_{i=1}^N k_i}{\sum_{i=1}^N n_i} \\
s^2 = \frac{1}{N}\sum_{i=1}^N var \left(\frac{k_i}{n_1} \right) \\
\hat{M}  = \frac{\hat{\mu}(1-\hat{\mu})-s^2}{s^2 - \frac{\hat{\mu}(1-\hat{\mu}}{N} \sum_{i=1}^N \frac{1}{n_i}}
$$


Once we have the point estimates for the underlying distribution, we use them to find a point estimate of $\hat{\theta_i}$, the probability of success for event $i$: 
$$
\hat{\theta_i} = \frac{k_i + \hat{M}\hat{\mu}}{n_i + \hat{M}}
$$ 

```{r}
N <- nrow(all_SAR)

# point estimates for mu and M
hat_theta = all_SAR$secondary / all_SAR$num_contacts
mu = sum(all_SAR$secondary)/sum(all_SAR$num_contacts)
s2 = N *  sum(all_SAR$num_contacts * (hat_theta -mu)^2)/((N-1) * sum(all_SAR$num_contacts))
M_hat = (mu * (1-mu) -s2)/(s2 - mu * (1-mu)/N * sum(1/all_SAR$num_contacts))

print(paste("M_hat=", M_hat, "mu=", mu))

alpha <- M_hat*mu
beta <- M_hat*(1-mu)
print(paste("alpha=", alpha, "beta=", beta))

# point estimate for the posterior (probability of success for event i)
theta_i <- (all_SAR$secondary + M_hat*mu)/(all_SAR$num_contacts + M_hat)
all_SAR$theta_i <- theta_i


```

```{r}
p <- seq(0,1,length=100)
pdf(file.path(path.out.figures, "beta_dist.pdf"), width =8, height =5)
plot(p, dbeta(p, alpha, beta), ylab="density", type ="l", col=7)
legend(0.5,5, c("Be(alpha=0.70, beta=2.80)"),
       lty=c(1,1,1,1),col=c(7,6,5,4,3,2,1))
dev.off()
```


Simulate number of infections using fitted distribution, compare to observed
```{r}
B <- 1000
mock <- sapply(1:B, function(b){
  u = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,all_SAR$theta_i)/x}) 
  o = ks.test(u, all_SAR$SAR)
  return(o$p.value)
})
hist(mock)

# just to visualize- not a formal test
sim_dist <- lapply(all_SAR$num_contacts, function(x){rbinom(100,x,all_SAR$theta_i)})
sim_dist <- data.frame(x=unlist(sim_dist))

pdf(file.path(path.out.figures, "fitted_tau_dist.pdf"), width =8, height =5)
ggplot()+
  geom_density(data=sim_dist, aes(x=x), alpha=0.3, fill="red")+
  geom_density(data=all_SAR, aes(x=secondary), alpha=0.3, fill="green")+
  ggtitle("Simulated Secondary Cases vs. Observed")
dev.off()

p <- seq(0,1,length=100)
plot(p, dbeta(p, alpha, beta), ylab="density", type ="l", col=7)
lines(p, dbeta(p, 0.1*alpha, 0.1*beta), type ="l", col=6)
lines(p, dbeta(p, 0.2*alpha, 0.2*beta), col=5) 
lines(p, dbeta(p, 0.3*alpha, 0.3*beta), col=4) 
lines(p, dbeta(p, 0.4*alpha, 0.4*beta), col=3) 
lines(p, dbeta(p, 0.5*alpha, 0.5*beta), col=2) 
lines(p, dbeta(p, 0.6*alpha, 0.6*beta), col=1) 
legend(0.7,6, c("Be(alpha, beta)","Be(.1a, .1b)",
                "Be(.2a,.2b)",  "Be(.3a, .3b)",
                "Be(.4a,.4b)",  "Be(.5a, .5b)", 
                "Be(.6a, .6b)"),
       lty=c(1,1,1,1),col=c(7,6,5,4,3,2,1))

p <- seq(0,1,length=100)
plot(p, dbeta(p, alpha, beta), ylab="density", type ="l", col=7)
lines(p, dbeta(p, 0.1*alpha, 0.1*beta), type ="l", col=6)
lines(p, dbeta(p, 0.2*alpha, 0.2*beta), col=5) 
lines(p, dbeta(p, 0.3*alpha, 0.3*beta), col=4) 
lines(p, dbeta(p, 0.4*alpha, 0.4*beta), col=3) 
lines(p, dbeta(p, 0.5*alpha, 0.5*beta), col=2) 
lines(p, dbeta(p, 0.6*alpha, 0.6*beta), col=1) 
legend(0.7,6, c("Be(alpha, beta)","Be(.1a, .1b)",
                "Be(.2a,.2b)",  "Be(.3a, .3b)",
                "Be(.4a,.4b)",  "Be(.5a, .5b)", 
                "Be(.6a, .6b)"),
       lty=c(1,1,1,1),col=c(7,6,5,4,3,2,1))

tauinv <- 1/c(.1,.2,.3,.4,.5,.6)

p <- seq(0,1,length=100)
plot(p, dbeta(p, alpha, beta), ylab="density", type ="l", col=7)
lines(p, dbeta(p, 0.9*alpha, 0.9*beta), type ="l", col=6)
lines(p, dbeta(p, 0.8*alpha, 0.8*beta), col=5) 
lines(p, dbeta(p, 0.7*alpha, 0.7*beta), col=4) 
lines(p, dbeta(p, 0.6*alpha, 0.6*beta), col=3) 
lines(p, dbeta(p, 0.5*alpha, 0.5*beta), col=2) 
lines(p, dbeta(p, 0.4*alpha, 0.4*beta), col=1) 
legend(0.6,6, c("Be(alpha, beta)","tau=.1, Be(.9a, .9b)",
                "tau=.2, Be(.8a,.8b)",  "tau=.3, Be(.7a, .7b)",
                "tau=.4, Be(.6a,.6b)",  "tau=.5, Be(.5a, .5b)", 
                "tau=.6, Be(.4a, .4b)"),
       lty=c(1,1,1,1),col=c(7,6,5,4,3,2,1))


```
TODO: need to access goodness of fit. CPO and PPO for Bayesian Goodness of Fit

TODO: Likelihood Ratio Test
```{r}
u = sapply(1:length(all_SAR$num_contacts), function(x){
  tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
  log(pbinom(all_SAR$secondary[x], all_SAR$num_contacts[x],tau))
})

likelihood <- sapply(1:B, function(b){
  uu = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] )}) 
  uuu = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
    log(pbinom(all_SAR$secondary[x], uu[x],tau ))
  })
  return( sum(u - uuu))
})

u0 = sapply(1:length(all_SAR$num_contacts), function(x){
  tau = 0.199
  log(pbinom(all_SAR$secondary[x], all_SAR$num_contacts[x],tau))
})

likelihood0 <- sapply(1:B, function(b){
  uu = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,0.199 )}) 
  uuu = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = 0.199   
    log(pbinom(all_SAR$secondary[x], uu[x],tau ))})
  return( sum(u0 - uuu))
})

likelihood2 <- sapply(1:B, function(b){
  uu = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] )}) 
  uuu = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
    log(pbinom(all_SAR$secondary[x], uu[x],tau ))
  })
  vv = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] )}) 
  vvv = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
    log(pbinom(all_SAR$secondary[x], vv[x],tau ))
  })
  return( c(sum(vvv - uuu)))
})

hist(likelihood)
```


```{r}
N = nrow(all_SAR)
hat_theta = all_SAR$secondary / all_SAR$num_contacts
mu = sum(all_SAR$secondary)/sum(all_SAR$num_contacts)
s2 = N *  sum(all_SAR$num_contacts * (hat_theta -mu)^2)/((N-1) * sum(all_SAR$num_contacts))
M_hat = (mu * (1-mu) -s2)/(s2 - mu * (1-mu)/N * sum(1/all_SAR$num_contacts))

```

```{r}
u = sapply(1:length(all_SAR$num_contacts), function(x){
  tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
  log(pbinom(all_SAR$secondary[x], all_SAR$num_contacts[x],tau))
           })



likelihood <- sapply(1:B, function(b){
  uu = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] )}) 
  uuu = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
  log(pbinom(all_SAR$secondary[x], uu[x],tau ))
})

  
  return( sum(u - uuu))
})


u0 = sapply(1:length(all_SAR$num_contacts), function(x){
  tau = 0.199
  log(pbinom(all_SAR$secondary[x], all_SAR$num_contacts[x],tau))
           })

likelihood0 <- sapply(1:B, function(b){
  uu = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,0.199 )}) 
  uuu = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = 0.199
  log(pbinom(all_SAR$secondary[x], uu[x],tau ))})
    return( sum(u0 - uuu))
})


likelihood2 <- sapply(1:B, function(b){
  uu = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] )}) 
  uuu = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
  log(pbinom(all_SAR$secondary[x], uu[x],tau ))
  })
  
  vv = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] )}) 
  vvv = sapply(1:length(all_SAR$num_contacts), function(x){
    tau = M_hat / (all_SAR$num_contacts[x] + M_hat) * mu +  all_SAR$num_contacts[x]/ ( all_SAR$num_contacts[x] + M_hat) * all_SAR$secondary[x]/all_SAR$num_contacts[x] 
  log(pbinom(all_SAR$secondary[x], vv[x],tau ))
  })

  return( c(sum(vvv - uuu)))
})

```


```{r}

all_SAR2 = all_SAR[which(all_SAR$secondary>0),]

##### Check goodness of fit
mock <- sapply(1:B, function(b){
  u = sapply(all_SAR2$num_contacts, function(x){rbinom(1,x,sum(all_SAR2$secondary)/sum(all_SAR2$num_contacts))/x}) + rnorm(length(all_SAR$num_contacts), 0, 0.001)
  #m = as.matrix(rbind(as.numeric(names(table(u))), table(u)))
  v = sapply(all_SAR2$num_contacts, function(x){rbinom(1,x,sum(all_SAR2$secondary)/sum(all_SAR2$num_contacts))/x})+ rnorm(length(all_SAR$num_contacts), 0, 0.001)
  #mm = as.matrix(rbind(as.numeric(names(table(v))), table(v)))
  o = ks.test(u, all_SAR$secondary/all_SAR$num_contacts + rnorm(length(all_SAR$num_contacts), 0, 0.001))
  oo = ks.test(u, v, )
  return(c(o$p.value, oo$p.value))
})

hist(mock[1,])
hist(mock[2,], alpha=0.5,add=TRUE)

tau2 = sum(all_SAR2$secondary)/sum(all_SAR2$num_contacts)
u = sapply(1:length(all_SAR2$num_contacts), function(x){
  log(npbinom(all_SAR2$secondary[x],all_SAR2$num_contacts[x],tau2))
})

likelihood <- sapply(1:B, function(b){
  uu = sapply(all_SAR2$num_contacts, function(x){rnbinom(1,x,tau2)}) 
  uuu = sapply(1:length(all_SAR2$num_contacts), function(x){
  log(pnbinom(all_SAR$secondary[x],uu[x],0.199))
})
  
  vv = sapply(all_SAR2$num_contacts, function(x){rnbinom(1,x,tau2)}) 
  vvv = sapply(1:length(all_SAR2$num_contacts), function(x){
  log(pnbinom(all_SAR$secondary[x],vv[x],0.199))
})
  
  return( c(exp(sum(u - uuu)), exp(sum(vvv - uuu))))
})
pchisq(likelihood, 1)
```



```{r}
##### Check goodness of fit (from Claire)
mock <- sapply(1:B, function(b){
  u = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,0.199)/x}) + rnorm(length(all_SAR$num_contacts), 0, 0.001)
  #m = as.matrix(rbind(as.numeric(names(table(u))), table(u)))
  v = sapply(all_SAR$num_contacts, function(x){rbinom(1,x,0.199)/x})+ rnorm(length(all_SAR$num_contacts), 0, 0.001)
  #mm = as.matrix(rbind(as.numeric(names(table(v))), table(v)))
  o = ks.test(u, all_SAR$secondary/all_SAR$num_contacts + rnorm(length(all_SAR$num_contacts), 0, 0.001))
  oo = ks.test(u, v, )
  return(c(o$p.value, oo$p.value))
})
hist(mock[1,])
hist(mock[2,], alpha=0.5,add=TRUE)
mean(mock[2,] < mean(mock[1,]))
```


The beta distribution is the conjugate prior probability distribution to the binomial and is a useful model for the random behavior of a percentage/proportion (since it is defined n the interval [0,1]). Given our observed SAR values (data, $X$) and a beta prior, we want to estimate the parameters (alpha and beta) 
may need: Bayesian updates (observing something sampled from binomial dist; want hyper parameters)
each tau is a RV sampled from beta, want to estimate alpha and beta
write down update from likelihood formula

```{r}
library(tidyverse)
source("beta_params.R")
sar_dat <- read.csv(file.path(path.in, "SAR_95CI_sc_collected.csv"))
sar_dat$alpha <- numeric(nrow(sar_dat)) 
sar_dat$beta <- numeric(nrow(sar_dat))

for (i in 1:nrow(sar_dat)){
  q <- c(sar_dat[i, "X95CI_lw"], sar_dat[i, "X95CI_up"])
  q[1] <- ifelse(q[1]==0, q[1]+0.0001, q[1]) # lower bound can't be exactly zero
  beta <- beta.parms.from.quantiles(q=q)
  sar_dat[i,"alpha"] <- beta$a
  sar_dat[i, "beta"] <- beta$b
}

# add alpha and beta that we calculated from Jing and Lewis data
# "alpha= 0.696031266386682 beta= 2.80459657338163"
sar_dat[nrow(sar_dat)+1,] <- c("LewisJing_2020", "Household", "NA", "NA", "NA", "sc_calculated",
                               0.696, 2.805)
sar_dat$alpha <- as.numeric(sar_dat$alpha)
sar_dat$beta <- as.numeric(sar_dat$beta)
# generate the beta distributions with different parameters
set.seed(123) # reproducible
p <- seq(0, 1, .01)
beta_dist <- matrix(nrow = length(p), ncol = nrow(sar_dat)+1)
beta_dist[,1] <- p
for(i in 1:nrow(sar_dat)){
  beta_dist[,(i+1)] <- dbeta(p, sar_dat[i, "alpha"], sar_dat[i, "beta"])
}
beta_dist <- data.frame(beta_dist)
beta_dist_long <- gather(beta_dist, p, dens, X2:X11, factor_key = TRUE)

ggplot(beta_dist_long, aes(x=X1, y = dens, group = p))+
  geom_line(aes(color = p))
```
```{r}
## Household
# Curmei et al 2020
q_house <- c(0.0001, .67)
beta_house <- beta.parms.from.quantiles(q=q_house)

# Wu et al. 2020
# household: 22.4%-44.4%
q_house1 <- c(.224, .444)
beta_house1 <- beta.parms.from.quantiles(q=q_house1)

## Restaurant / Meal (Koh et al 2020 meta analysis)
# Wu J, Huang Y, Tu C, Bi C, Chen Z, Luo L, et al. Household Transmission of SARS-CoV-2, Zhuhai, China, 2020. Clin Infect Dis. 2020.
# Shared meal: SIR 95% CI: 38.8(26.8-52.4)
q_meal <- c(.268, .524)
beta_meal <- beta.parms.from.quantiles(q=q_meal)

# Bi Q, Wu Y, Mei S, Ye C, Zou X, Zhang Z, et al. Epidemiology and transmission of COVID-19 in 391 cases and 1286 of their close contacts in Shenzhen, China: a retrospective cohort study. Lancet Infect Dis. 2020.
# shared meal: 8·6% (6·8–10·9)
q_meal2 <- c(.068, .109)
beta_meal2 <- beta.parms.from.quantiles(q=q_meal2)


## Travel
# Wu et al. 2020
# travel (shared car): 46.6(31.2-62.6)
q_travel <- c(0.312, 0.626)
beta_travel <- beta.parms.from.quantiles(q=q_travel)

# Bi et al 2020
# travel: 5·7% (3·6–8·8)
q_travel2 <- c(0.036, 0.088)
beta_travel2 <- beta.parms.from.quantiles(q=q_travel2)

p <- seq(0,1,length=100)
plot(p, dbeta(p, beta_meal$a, beta_meal$b), ylab="density", type ="l", col=7)
lines(p, dbeta(p, beta_house$a, beta_house$b), type ="l", col=6)
lines(p, dbeta(p, beta_meal2$a, beta_meal2$b), col=5) 
lines(p, dbeta(p, beta_travel$a, beta_travel$b), col=4) 
lines(p, dbeta(p, beta_travel2$a, beta_travel2$b), col=3) 
lines(p, dbeta(p, beta_house1$a, beta_house1$b), col=2) 

legend(0.6,6, c("Meal: Be(21, 33)", "House: Be(.45, 2.38", 
                "Meal2: Be(64, 663)", "Travel: Be(17, 20)", 
                "Travel2: Be(19, 294)", "House1: Be(23, 46"),
       lty=c(1,1,1,1),col=c(7,6,5,4, 3))
```


----------------------------------------------------------------------------

# Prevalence 

Better to use number of ACTIVE cases rather than number of new cases

model coefficient of variation (sd / mean): linear? fit distribution that matches these properties
don't take entire time series; generate coefficient of variation per week 
look at distribution of coefficient of variation as function of prevalence
If coefficient of variation is roughly linear, find distribution that roughly fits this criteria

### Active Case data
We actually want active cases, not new daily cases
```{r}
COUNTRY_DATA <- read.csv(file="https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv", header=T, sep=",")
# https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-codebook.csv
COUNTRY_DATA$date <- (as.Date(COUNTRY_DATA$date, "%Y-%m-%d"))

US_DATA <- COUNTRY_DATA %>% filter(location == "United States")
```

## TODO: use this dataset with smoothed values to get activecases (new_cases_smoothed)
plot coefficient of variation of new_cases_smoothed 
want to know: how much does variance change as a function of prevalence?
modeling uncertainty based on data
```{r}
country_new <- COUNTRY_DATA %>%
   # dplyr::arrange(desc(state)) %>% 
    #dplyr::group_by(state) %>% 
  dplyr::mutate(activecases=zoo::rollapply(new_cases_smoothed_per_million, 
                                           width=14, 
                                           FUN=function(x) sum(x, na.rm=FALSE), 
                                           align="right", fill=NA),
                activecases_sd=zoo::rollapply(activecases,
                                                        width=7, 
                                                        FUN=function(x) sd(x, na.rm=FALSE),
                                                        align="right", fill=NA), 
                activecases_coefvar = activecases_sd/activecases,
                year = factor(as.numeric(format(date, format = "%Y"))),
                month = factor(as.numeric(format(date, format = "%m"))),
                day = as.numeric(format(date, format = "%d"))
                ) 

# data cleaning
## remove countries that have negative values for the coefficient of variation
bad <- country_new %>% filter(activecases_coefvar < 0) %>% dplyr::select(location) %>% unique()
bad_index <- country_new$location %in% bad$location
country_clean <- country_new[!bad_index,]

pi_dist1 <- ggplot(country_clean %>% filter(activecases_coefvar < 5 & location == "United States"), aes(x=activecases, y=activecases_coefvar))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Active Cases (7-Day Rolling Avg.)")+
  ylab("Coefficient of Variation, 7-Day Rolling Avg.")+
  ggtitle("United States")

pi_dist2 <- ggplot(country_clean %>% filter(activecases_coefvar < 5 & year==2021 & location == "United States"), aes(x=activecases, y=activecases_coefvar))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Active Cases (7-Day Rolling Avg.)")+
  ylab("Coefficient of Variation, 7-Day Rolling Avg.")+
  ggtitle("United States")

pi_dist3 <- ggplot(country_clean %>% filter(activecases_coefvar < 5 & year == 2021), aes(x=activecases, y=activecases_coefvar))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Active Cases (7-Day Rolling Avg.)")+
  ylab("Coefficient of Variation, 7-Day Rolling Avg.")+
  ggtitle("Global")

pdf(file.path(path.out.figures, "coefvariation_global.pdf"), width =8, height =5)
pi_dist3
dev.off()

pdf(file.path(path.out.figures, "coefvariation_US.pdf"), width =8, height =5)
pi_dist1
pi_dist2
dev.off()

```





## New Cases (don't use)
Get data from COVID Act Now API: https://apidocs.covidactnow.org/api
COVID Act Now, API key: f21877deb3c9401399d1584884d175a8
Data definitions available here: https://apidocs.covidactnow.org/data-definitions


## All state timeseries and US time series data
```{r}
# NEW and CUMULATIVE Case data
# from CovidActNow
#dat <- read.csv("covidactnow.states.timeseries.csv")
# states_timeseries <- read.csv("https://api.covidactnow.org/v2/states.timeseries.csv?apiKey=f21877deb3c9401399d1584884d175a8")
us_timeseries <- read.csv("https://api.covidactnow.org/v2/country/US.timeseries.csv?apiKey=f21877deb3c9401399d1584884d175a8")

# Download alternative data: https://covidtracking.com/data
#allstates_rollingavg <- read.csv("all-states-history_covidtrackingproject.csv")
```

```{r}
# remove unnecessary columns
us_ts <- us_timeseries %>% dplyr::select(date, contains("cases"), contains("case"))

# remove dates where actuals.newCases is missing (NA)
us_ts <- us_ts[!is.na(us_ts$actuals.newCases),]
```


- Compute seven-day rolling average of cases for the US overall
- `metrics.caseDensity` gives the number of cases per 100k population calculated using a 7-day rolling average. Compute the standard deviation of this rolling average.
- Compute standard deviation of seven day rolling avg. and `metrics.caseDensity`
```{r}
us_ts$date <- as.Date(us_ts$date)
us_ts <- us_ts %>%
   # dplyr::arrange(desc(state)) %>% 
    #dplyr::group_by(state) %>% 
  dplyr::mutate(activecases=zoo::rollapply(actuals.newCases, 
                                           width=14, 
                                           FUN=function(x) sum(x, na.rm=FALSE), 
                                           align="right", fill=NA),
                activecases_7davg=zoo::rollapply(activecases,
                                                        width=7, 
                                                        FUN=function(x) mean(x, na.rm=FALSE),
                                                        align="right", fill=NA), 
                activecases_sd=zoo::rollapply(activecases_7davg,
                                                        width=7, 
                                                        FUN=function(x) sd(x, na.rm=FALSE),
                                                        align="right", fill=NA), 
                activecases_coefvar = activecases_sd/activecases_7davg,
                cases_03da = zoo::rollmean(actuals.newCases, k = 3, fill = NA),
                cases_05da = zoo::rollmean(actuals.newCases, k = 5, fill = NA),
                cases_07da = zoo::rollmean(actuals.newCases, k = 7, fill = NA),
                cases_15da = zoo::rollmean(actuals.newCases, k = 15, fill = NA),
                cases_21da = zoo::rollmean(actuals.newCases, k = 21, fill = NA), 
                cases_07da_sd = zoo::rollapply(cases_07da, width=7, 
                                               FUN=function(x) sd(x, na.rm=FALSE), 
                                               by =1, fill=NA), 
                metrics.caseDensity_sd = zoo::rollapply(metrics.caseDensity,
                                                        width=7, 
                                                        FUN=function(x) sd(x, na.rm=FALSE),
                                                        align="right", fill=NA), 
                metrics.caseDensity_coefvar = metrics.caseDensity_sd/metrics.caseDensity,
                # coefficient of variation, rolling seven day
                year = factor(as.numeric(format(date, format = "%Y"))),
                month = factor(as.numeric(format(date, format = "%m"))),
                day = as.numeric(format(date, format = "%d"))) 
#%>% dplyr::ungroup()

#t <- us_timeseries[is.na(us_timeseries$cases_07da),]
#t2 <- us_timeseries[is.na(us_timeseries$actuals.newCases),]
```

Plot standard deviation as a function of prevalence
```{r}
ggplot(us_ts, aes(x=cases_07da, y=cases_07da_sd))+
  geom_line()

ggplot(us_ts, aes(x=metrics.caseDensity, y=metrics.caseDensity_sd))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Cases per 100,000 (7-Day Rolling Avg.)")+
  ylab("Std. Dev. of Cases per 100,000 7-Day Rolling Avg.")

ggplot(us_ts, aes(x=metrics.caseDensity, y=metrics.caseDensity_sd))+
  geom_point(aes(color=month)) +
  facet_wrap(~year)+
  xlab("Cases per 100,000 (7-Day Rolling Avg.)")+
  ylab("Std. Dev. of Cases per 100,000 7-Day Rolling Avg.")
```

We observe that the higher the prevalence, the higher the sd. So variance is heteroscedastic. We want a model that reflects this. 

look at distribution of coefficient of variation as function of prevalence
If coefficient of variation is roughly linear, find distribution that roughly fits this criteria
```{r}
ggplot(us_ts, aes(x=metrics.caseDensity, y=metrics.caseDensity_coefvar))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Cases per 100,000 (7-Day Rolling Avg.)")+
  ylab("Coefficient of Variation, 7-Day Rolling Avg.")

ggplot(us_ts, aes(x=activecases_7davg, y=activecases_coefvar))+
  geom_point(aes(color=month, shape = year)) +
  xlab("Active Cases (7-Day Rolling Avg.)")+
  ylab("Coefficient of Variation, 7-Day Rolling Avg.")
```


Fit distributions to the real data 
```{r}
# remove missing metrics.caseDensity_sd values
# (missing for first and last 3 days of data set)
density_sd_dat <- us_ts[,"metrics.caseDensity_sd"]
density_sd_dat <- density_sd_dat[!is.na(density_sd_dat)]


# Fit distribution
# Plot distribution and skew/kurtosis of standard deviation of the case density
plotdist(density_sd_dat, histo = TRUE, demp = TRUE, breaks=18)
descdist(density_sd_dat, boot = 1000)

# Fit distributions
fw <- fitdist(density_sd_dat, "weibull")
fno <- fitdist(density_sd_dat, "norm")
fg <- fitdist(density_sd_dat, "gamma")
fln <- fitdist(density_sd_dat, "lnorm")

# Plot fit distributions and generate goodness-of-fit statistics
par(mfrow = c(2, 2))
plot.legend <- c("weibull","normal","gamma", "lnorm")
denscomp(list(fw,fno,fg,fln), legendtext = plot.legend)
qqcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
cdfcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
ppcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
gofstat(list(fw,fno,fg,fln))

## Beta distribution
# rescale data in interval [0,1] to fit beta distribution
# density_sd_dat_scaled <- density_sd_dat/10 
# # scaling by max(density_sd_dat) leads to fitdit() failing; 10 chosen somewhat aribitrarily
# fit all the distributions on the scaled data so that the results are comparable
# fb <- fitdistrplus::fitdist(density_sd_dat_scaled, "beta")
# fw <- fitdist(density_sd_dat_scaled, "weibull")
# fno <- fitdist(density_sd_dat_scaled, "norm")
# fg <- fitdist(density_sd_dat_scaled, "gamma")
# fln <- fitdist(density_sd_dat_scaled, "lnorm")

# par(mfrow = c(2, 2))
# plot.legend <- c("weibull","normal","gamma", "lnorm", "beta)
# denscomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend,xlim=c(0,50))
# qqcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# cdfcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# ppcomp(list(fw,fno,fg,fln, fb), legendtext = plot.legend)
# gofstat(list(fw,fno,fg,fln, fb))

## beta ends up being very similar to the other distributions and does not perform any better
## so lets go back to the unscaled data  (as above)
```
Log Normal (`lnorm`) has best fit based on the fit statistics. Extract fit parameters. 
```{r get_best_fit}
summary(fln)
meanlog= summary(fln)[[1]][1]
sdlog = summary(fln)[[1]][2]
```

CHECK: The log-normal distribution is often parameterised by the parameters for the corresponding normal distribution, hence why the estimate of the mean is negative. So we can generate data $Y \sim \mathcal{N}(\mu, \sigma^2)$, and then find the corresponding log-normal $X = exp(Y)$. 

https://stats.stackexchange.com/questions/126397/lognormal-with-negative-mean

Fitting parametric distributions in R: 
https://www.r-project.org/conferences/useR-2009/slides/Delignette-Muller+Pouillot+Denis.pdf




