---
title: "Correlated Individuals Experiments"
author: "Claire Donnat, Saskia Comess"
date: "December 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(fitdistrplus)
library(DescTools)
library(data.table)
library(dplyr)
```

# Load the real data and extract the distribution parameters
```{r loading}
# Get all tests
tests <- data.table::fread("alltests_1mar24jun_v1.csv")

# Keep only valid positive ct values for first tests
tests %>% filter(result == "positive", firsttest==TRUE,!is.na(cttarget)) %>% pull(cttarget) -> cts

# Fit distribution
# Plot distribution and skew/kurtosis of ct values of real first tests
plotdist(cts, histo = TRUE, demp = TRUE, breaks=18)
descdist(cts, boot = 1000)

# Fit distributions
fw <- fitdist(cts, "weibull")
fno <- fitdist(cts, "norm")
fg <- fitdist(cts, "gamma")
fln <- fitdist(cts, "lnorm")

# Plot fit distributions and generate goodness-of-fit statistics
par(mfrow = c(2, 2))
plot.legend <- c("weibull","normal","gamma", "lnorm")
denscomp(list(fw,fno,fg,fln), legendtext = plot.legend,xlim=c(0,50))
qqcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
cdfcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
ppcomp(list(fw,fno,fg,fln), legendtext = plot.legend)
gofstat(list(fw,fno,fg,fln))

```

Weibull has best fit. Extract fit parameters. We can thus assume that the Ct values are sampled from a Weibull distribution with shape 4.55 and scale 29.86.
```{r get_best_fit}
summary(fw)
shape_w = summary(fw)[[1]][1]
scale_w = summary(fw)[[1]][2]
```

# Generate (un-correlated) Ct data
The viral load distribution changes from population to population. As such, we need to take this variability into account: we consider as a surrogate for this variability the proportion of variables that are above the Limit of Detection.

To achieve this, we simply jiggle the distribution:
- Step 1: we sample 50,000 samples from our fitted Weibul distribution
- Step 2: for a fixed lod = 35, we create a dictionary between the jiggling and the quantile associated to 35.
- Step 3: we create a second dictionary mapping the percentage above the threshold and the correct jiggling.

```{r}
## GENERATE UN-CORRELATED Ct DATA
# Add variable of proportion Ct value >LoD to model, as surrogate for differences in viral load distribution in different populations

# Number of replicates to generate ecdf offset values for above.
set.seed(42)
# x<-50000
x<-5000

# Determine ct value quantiles for real first tests vs. fitted Weibull distribution
quantile(cts,c(0.5,0.25,0.75,0.95,0.99))
quantile(fw,c(0.5,0.25,0.75,0.95,0.99))

# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
# 5000 draws from weibull with shape and scale parameters drawn from real data
ct_fake_input <- rweibull(x,fw[[1]][1],fw[[1]][2]) 

# Create matrix of desired input parameters
# Change above.lod to % samples with ct value >LoD to reflect actual population of interest. Changing "lod" itself has no effect on model output.
lod <- 35
above.lod <- seq(0.05,0.3,0.05)
translation_vector <- seq(-10,15,0.01)
mat<-matrix(ncol=3,nrow=length(translation_vector))

# Loop 1: fix LOD; for known shift, what percent of samples are above LoD (a)? 
for(v in 1:length(translation_vector)){
  i=1 
  fn<-ecdf(subset(pmax(5,ct_fake_input+translation_vector[v]),
                  (pmax(5,ct_fake_input+translation_vector[v]))<45)) ## selects adequate values within the sample
			a<-1-fn(lod) ## percent of cvalues in the distribution above the LoD 
			mat[(v-1)+i,1]<-lod
			mat[(v-1)+i,2]<-a
			mat[(v-1)+i,3]<-translation_vector[v]
}

# Shift ct values for each LoD and %above lod
mat2<-matrix(ncol=3,nrow=length(above.lod))

# Loop 2: fix LOD; for known a, how much should you shift? 
for(j in 1:length(above.lod)){
	tmp<-subset(mat,mat[,1]==lod)
	u<-tmp[which.min(abs(above.lod[j]-tmp[,2])),3]
	mat2[j,1]<-lod[i]
	mat2[j,2]<-above.lod[j]
	mat2[j,3]<-u
}
```

# Include the probit  coefficients
The probits scores are necessary to compute the viral load.
```{r}
# probit data input
probit_input <- read.csv("probit_zscores_cts_tissue_agnostic.csv")
probit_t<-subset(probit_input,probit_input$z_value<=1.96 & probit_input$z_value>=-1.96)
probit<-probit_t[,c(2,4,3)]
probit<-probit[order(probit$z_value,probit$ct_value),]
z_scores<-as.numeric(unlist(distinct(probit,z_value)))

# Number of replicates for model
set.seed(42)
n <- 10000

pool.max<-20
probit.mode<-c("base","dsa.lower","dsa.upper","psa") 
probit.mode.index<-1 # 1 = no variation, 2, = LLN, 3 = ULN, 4 = probabilistic
probit.z.indices<-c(488,1,length(z_scores)) # 488 is a z score of 0 (base case) in the z index vector
dilution.vary.index<-1 # 1 = no variation, 2 = probabilistic
```

# Loops to vary % above each LOD for different combinations of pool size, prevalence, and positives
## experiment_loop: Correlated Individuals

experiment_loop() calculates the weighted means using null probabilities (assuming no correlation between individuals, binomial distribution) and using simulated probabilities (accounting for correlated individuals, probability of transmission within network specified by min, max in the calc_probs() fxn) 
```{r correlated_individuals, message=FALSE}
# Model loop, vary % above each LOD (simplified from original code)

# source("MCMC_sim.R") 
# contains calc_probs() fxn. This SIMULATES the number of positives per pool

source("compute_probas.R") 
# contains exact_probs fxn. This CALCULATES the number of positives per pool 
pool.max = 100
TAU_RELATIVE_VAR = 5
# if you want to use the simulated probabilities that don't account for community infections (network only)
# use this to get model_output_corr_indiv_.1.2.csv from experiment_loop()
# sim_probs_.1.2 <- read.csv("sim_probs_.1.2.csv")
# sim_probs_.1.2[,1] <- NULL


experiment_loop <- function(above, ct_dat, sim_min, sim_max){
  ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45)) 
  # (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
  # (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
  threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
  rbindlist(lapply(1:pool.max, function (p) { # pool size
    rbindlist(lapply(c(0.0001,0.001,0.005, 0.01, 0.03, 0.05, 0.1, 0.2), function(prevalence) { # prevalence ("proportion positive tests")
      # sim_probs <- calc_probs_all_homogeneous(p, prevalence, min=sim_min, max=sim_max, B=1000)
      # specify range of correlation to sample from (min, max)
      print(prevalence)
      rbindlist(lapply(c(0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5,  0.7), function(tau) {
        print(tau)
        sim_probs <- compute_probas_level2_variation(p, prevalence, tau, TAU_RELATIVE_VAR)
        rbindlist(lapply(0:p, function(positives) { # number of positives
          #print(positives)
           
           if (positives == 0) {
            # prevalence_corr <-  1-((1-prevalence)*(1-(prevalence*tau))^(p-1))
            data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                       above.llod = above.lod[above], 
                       concentration=0, 
                       tau = tau,
                       tau_relative_var = TAU_RELATIVE_VAR,
                       n_eff = unique(sim_probs$n_eff),
                       probability = as.numeric(sim_probs[which(sim_probs$n == 0), "p"]),
                       #probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
                       probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
                       prevalence_corr = sim_probs$pi_eff[1],
                       probability_null = sim_probs$prob_null_eff[1], # prevalence corrected for correlated individuals
                       #random=0, 
                       z.index=0,
                       call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
          } 
          else {
            dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives) 
            # sample data uniformly at random 
            # n samples of positives, rearrange into a matrix of positive rows, n columns
            each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
                                                        rnorm(mean=0,sd=1.1,n=ncol(dat))) 
            # sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
            # calculation dilution based on number of positives (colSum) in total pool size (p)
            z.index= probit.z.indices[probit.mode.index]

           t = data.frame(
              limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                       above.llod = above.lod[above], 
                       concentration=0, 
                       tau = tau,
                       tau_relative_var = TAU_RELATIVE_VAR,
                       n_eff = sim_probs$n_eff[1],
                       probability = as.numeric(sim_probs[which(sim_probs$n == positives), "p"]),
                       #probability_subset1 =  sim_subset_probs[which(sim_subset_probs$n == 0), "p"],
                       probability_null_test = dbinom(positives, p, prevalence), # uncorrected prevalence
                       prevalence_corr = sim_probs$pi_eff,
                       probability_null = sim_probs$prob_null_eff, # prevalence corrected for correlated individuals
              #random=sample(n,n,replace = TRUE), #
              z.index= z.index)
            t["call.each.conc"] = mean(probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2])
            t["tests"]=1 + p * (t["call.each.conc"] ) # number of tests done (number positive pools + pool size)
            t["tn"] = 0
            t["tp"] = 1 * (t["call.each.conc"] )
            t["fn"] =1 * (1-t["call.each.conc"] )
            t["fp"]=0
            return(t)
          }
        }))
      }))
    }))
  }))
}
```





```{r}
# apply the model loop to the fake correlated data
#allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, sim_min = 0.1, sim_max = 0.2, ct_dat = ct_fake_input))
allci <- rbindlist(lapply(3, experiment_loop, ct_dat = ct_fake_input)) # just for 15% Ct > LoD 

group_by(allfirst.poolct, pool, prevalence, above.llod, limit, tau) %>%
  summarize(n_eff = mean(n_eff),
            pos1=weighted.mean(pos, w=probability),
            pos_null=weighted.mean(pos, w=probability_null),
            total.tests=weighted.mean(tests, w=probability),
            total.tests_null=weighted.mean(tests, w=probability_null),
            tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
            tests.per.sample_null=weighted.mean(tests, w=probability_null)/mean(pool),
            tn1=weighted.mean(tn, w=probability),
            tp1=weighted.mean(tp, w=probability),
            fn1=weighted.mean(fn, w=probability),
            fp1=weighted.mean(fp, w=probability),
            ppa = weighted.mean(tp/(fn + tp), w=probability, na.rm = TRUE),
            ppa_null = weighted.mean(tp/(fn + tp), w=probability_null, na.rm = TRUE),
            tn_null=weighted.mean(tn, w=probability_null),
            tp_null=weighted.mean(tp, w=probability_null),
            fn_null=weighted.mean(fn, w=probability_null),
            fp_null=weighted.mean(fp, w=probability_null),
            ppa_null_observed = weighted.mean(tp/(fn + tp), w=probability_null_test, na.rm = TRUE),
            tests.per.sample_null_observed=weighted.mean(tests, w=probability_null_test)/mean(pool),
            tn_null_observed=weighted.mean(tn, w=probability_null_test),
            tp_null_observed=weighted.mean(tp, w=probability_null_test),
            fn_null_observed =weighted.mean(fn, w=probability_null_test),
            fp_null_observed =weighted.mean(fp, w=probability_null_test)) -> apw
# rm(allfirst.poolct)

allci <- as.data.frame(apw)
# # rm(apw)
# 
 all2 <- all %>% mutate(ppa=tp/(tp+fn))
cpi <- as_tibble(BinomCI(all2$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
# #allci <- bind_cols(all2,cpi)
# 
# cpi <- as_tibble(BinomCI(all$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
# allci <- bind_cols(all,cpi)
# write.csv(allci, "correlated_exactprobs_Ct15.csv") 
# write.csv(allci,"correlated_individuals_rho.1.2.csv")
# write.csv(allci,"correlated_individuals_rho.4.6.csv")

#correlated_individuals_rho.1.2, calc_probs(p, prevalence, min=0.1, max=0.2, B=1000)
```
```{r}
probs<-table(apply(sapply(prevs, function(x){rbinom(10000,1,x)}),1,sum))/10000

compute_p_all_levels <- function(N, prevs, tau, B=10000){
    #### when both prev and tau are vectors
    probs <- factor(apply(sapply(prevs, function(x){rbinom(B,1,x)}),1,sum), levels= 0:N)
    probs = as.data.frame(table(probs))
    probs$Freq = probs$Freq/B
    A = 0
    for (k in 1:(N-1)){
      Bb = exp(sum(sapply(1:(K-k), function(x){log(1-exp(sum(log(1-tau[sample(1:length(tau), k)]))))})))
      C = exp(sum(log(sapply(1:(N-K), function(x){exp(sum(log(1-tau[sample(1:length(tau), k)])))}))))
      A = A+ probs[which(probs$probs ==  k), "Freq"]* choose(N-k, K- k) * Bb * C  
  #   #print(A)
   }
  return(mean(prevs) + (1-mean(prevs))*A)
}

compute_probas_level_all_variations <- function(N, prev, tau, tau_relative_var, prev_relative_var, B=10000){

  probs <- factor(apply(sapply(prevs, function(x){rbinom(B,1,x)}),1,sum), levels= 0:N)
  probs = as.data.frame(table(probs))
  probs$Freq = probs$Freq/B
  simulations<- rbindlist(lapply(1:B, function(b){
  tilde_tau = rnorm(N^2, log(tau/(1-tau)), sd=log(tau_relative_var)/2)
  tilde_tau  = 1/(1+exp(- tilde_tau))
  tilde_prev = rnorm(N, log(prev/(1-prev)), sd=log(prev_relative_var)/2)
  tilde_prev  = 1/(1+exp(- tilde_prev))
  pi_eff = compute_p_all_levels(N,tilde_prev,tilde_tau)
  rho = compute_corr_all_levels(N,tilde_prev,tilde_tau)
  res = rep(0, N+1)
  res[1] = exp(sum(log(1-prev)))
  
  for (K in 1:N){
    for (k in 1:K){
      Bb = exp(sum(sapply(1:(K-k), function(x){log(1-exp(sum(log(1-tau[sample(1:length(tau), k)]))))})))
      C = exp(sum(log(sapply(1:(N-K), function(x){exp(sum(log(1-tau[sample(1:length(tau), k)])))}))))
      res[K+1] = res[K+1]  + probs[which(probs$probs ==  k), "Freq"]* choose(N-k, K- k) * Bb * C
    }
  }
  res_temp = data.frame(n = 0:N,
                        p = res,
                        pi_eff = pi_eff,
                        n_eff= N/(1+(N-1) *rho),
                        prob_null_eff = sapply(0:N, function(positives){dbinom(positives, N, pi_eff )}))
  
  }))
  return(simulations %>% group_by(n) %>% summarize_all(mean))
}


```

```{r}
allci <- allfirst.poolct

```

```{r}
#allci <- read.csv("correlated_exactprobs_Ct15.csv")
ppa_plot <- ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence),linetype="correlated")) +
  geom_line(allci, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence),linetype="null")) +
  facet_grid(cols = vars(factor(allci$tau))) +
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  # xlab("Pool size") + 
  ylab("Expected PPA") +
  # ggtitle("Correlated vs. IID Samples") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(limits=c(0.8,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Community Prevalence")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 


# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot_test.pdf",width=8, height=5)  
tests_plot <- ggplot() +
  geom_line(allci, mapping = aes(x=n_eff, y= tests.per.sample, color=as.factor(prevalence), linetype = "correlated")) +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = "null")) +
  facet_grid(cols = vars(factor(allci$tau))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Community Prevalence")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
#dev.off()

library(ggpubr)
#pdf("plot_test.pdf",width=8, height=5)  
print(ggarrange(ppa_plot, tests_plot, 
          labels = c("A", "B"), 
          ncol=1, nrow = 2, 
          common.legend = TRUE, legend = "right"))
#dev.off()
```
```{r}
#allci <- read.csv("correlated_exactprobs_Ct15.csv")
ppa_plot <- ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence),linetype="correlated")) +
  geom_line(allci, mapping = aes(x=pool, y=ppa_null_observed, color=as.factor(prevalence),linetype="null")) +
  facet_grid(cols = vars(factor(allci$tau, labels = c("Tau = 0.1", "0.2", "0.3", "0.4", "0.5")))) +
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  # xlab("Pool size") + 
  ylab("Expected PPA") +
  # ggtitle("Correlated vs. IID Samples") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(limits=c(0.8,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Community Prevalence")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 


# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot_test.pdf",width=8, height=5)  
tests_plot <- ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = "correlated")) +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample_null_observed, color=as.factor(prevalence), linetype = "null")) +
  facet_grid(cols = vars(factor(allci$tau))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,4)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Community Prevalence")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
#dev.off()

library(ggpubr)
#pdf("plot_test.pdf",width=8, height=5)  
print(ggarrange(ppa_plot, tests_plot, 
          labels = c("A", "B"), 
          ncol=1, nrow = 2, 
          common.legend = TRUE, legend = "right"))
#dev.off()
```


```{r}
# quick check: should have same results for null and alternative when pool size is 1
allci %>%
  filter(pool == 1) %>%
  dplyr::select(ppa, ppa_null)


# quick check: should have same results for null and alternative when pool size is 1
allci %>%
  filter(pool == 1) %>%
  dplyr::select(tests.per.sample, tests.per.sample_null, tau)

table(is.na(allfirst.poolct %>%
  filter(pool == 1) %>%
  dplyr::select(probability, probability_null)))

t <- allfirst.poolct %>%
  filter(pos== 0 | pos == 1) %>%
  dplyr::select(pool, pos, prevalence, probability, probability_null)

```





```{r}
# table to nummerically summarize some of the graphical results
summary_table <- allci %>%
  mutate(ppa_diff=ppa - ppa_null, 
         test_diff=tests.per.sample - tests.per.sample_null) %>%
  filter(pool == 4 | pool == 18) %>%
  filter(prevalence == 0.001 | prevalence == 0.1) %>%
  dplyr::select(pool, prevalence, tau, ppa_diff, ppa_null, test_diff, tests.per.sample_null)

write.csv(summary_table, "summary_table_ppa.csv")
```






```{r}

ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence),linetype="correlated")) +
  geom_line(allci, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence),linetype="null")) +
  facet_wrap(vars(factor(allci$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Correlated Individuals (rho = 0.4 - 0.6) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 


# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot.eff.corrindiv.4.6.pdf",width=8, height=5)  
ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = "correlated")) +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = "null")) +
  facet_wrap(vars(factor(allci$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  ggtitle("Correlated Individuals (rho = 0.4 - 0.6) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
#dev.off()
```



Plots for Rho = (0.1, 0.2)
```{r}
# corr_indiv_.1.2 <- read.csv("correlated_individuals_rho.1.2.csv")
# corr_indiv_.1.2_null <- corr_indiv_.1.2
# corr_indiv_.1.2$null <- "Correlated"
# corr_indiv_.1.2_null$null <- "Uncorrelated"
corr_indiv_.1.2 <- read.csv("test.csv")
corr_indiv_.1.2_null <- corr_indiv_.1.2
corr_indiv_.1.2$null <- "Correlated"
corr_indiv_.1.2_null$null <- "Uncorrelated"

pdf("plot.ppa.corrindiv.pdf",width=8, height=5)  
ggplot() +
  geom_line(corr_indiv_.1.2, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence), linetype = null)) +
  # y= tp1/(tp1 + fn1)
  geom_line(corr_indiv_.1.2_null, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence), linetype = null)) +
  # y=tp_null/(tp_null + fn_null)
  facet_wrap(vars(factor(corr_indiv_.1.2$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Correlated Individuals (rho = 0.1 - 0.2) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.corrindiv.pdf",width=8, height=5)  
ggplot() +
  geom_line(corr_indiv_.1.2, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = null)) +
  geom_line(corr_indiv_.1.2_null, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = null)) +
  facet_wrap(vars(factor(corr_indiv_.1.2$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  ggtitle("Correlated Individuals (rho = 0.1 - 0.2) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

```

Plots for Rho = (0.4, 0.6)
```{r}
corr_indiv_.4.6 <- read.csv("correlated_individuals_rho.4.6.csv")
corr_indiv_.4.6_null <- corr_indiv_.4.6
corr_indiv_.4.6$null <- "Correlated"
corr_indiv_.4.6_null$null <- "Uncorrelated"

pdf("plot.ppa.corrindiv.4.6.pdf",width=8, height=5)  
ggplot() +
  geom_line(corr_indiv_.4.6, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence), linetype = null)) +
  geom_line(corr_indiv_.4.6_null, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence), linetype = null)) +
  facet_wrap(vars(factor(corr_indiv_.4.6$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Correlated Individuals (rho = 0.4 - 0.6) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.corrindiv.4.6.pdf",width=8, height=5)  
ggplot() +
  geom_line(corr_indiv_.4.6, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = null)) +
  geom_line(corr_indiv_.4.6_null, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = null)) +
  facet_wrap(vars(factor(corr_indiv_.4.6$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  ggtitle("Correlated Individuals (rho = 0.4 - 0.6) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

```

## test: correlated individuals populate entire group
```{r correlated_individuals}
source("MCMC_SCmodified.R")
experiment_loop <- function(above, ct_dat, sim_min, sim_max){
  ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45)) 
  # (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
  # (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)
  threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
  
  rbindlist(lapply(1:pool.max, function (p) { # pool size
    rbindlist(lapply(c(0.001,0.01,0.05,0.1, 0.15), function(prevalence) { # prevalence ("proportion positive tests")
      # 0.001,0.01,0.05,0.1, 0.15
      sim_probs <- simulate_infections(p, 0,min=sim_min, max=sim_max, prev=rep(prevalence,p), B=5000)
      # specify range of correlation to sample from (min, max)
      rbindlist(lapply(0:p, function(positives) { # number of positives
        
        if (positives == 0) {
          data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                     above.llod = above.lod[above], 
                     concentration=0, 
                     probability = sim_probs[which(sim_probs$n == 0), "Freq"],
                     probability_null = dbinom(positives, p, prevalence), 
                     random=0, z.index=0,
                     call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
        } 
        else {
          dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives) 
          # sample data uniformly at random 
          # n samples of positives, rearrange into a matrix of positive rows, n columns
          
          each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,
                                                      rnorm(mean=0,sd=1.1,n=ncol(dat))) 
          # sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
          # calculation dilution based on number of positives (colSum) in total pool size (p)
          
          data.frame(
            limit=lod,
            pool=p,
            pos=positives,
            prevalence=prevalence,
            above.llod=above.lod[above],
            concentration=each.conc,
            # probability = (1-(1-prevalence)^p)*sim_probs_.1.2[positives, (p-1)], # using network only infection probs
            probability = sim_probs[which(sim_probs$n == positives), "Freq"]/n,
            probability_null = dbinom(positives, p, prevalence)/n,
            random=sample(n,n,replace = TRUE)/n, #
            z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
                           sample(1:length(z_scores),n,replace=T))) %>%
            mutate(
              call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
              tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
              tn=0,
              tp=1 * (call.each.conc),
              fn=1 * (!call.each.conc),
              fp=0)
        }
      }))
    }))
  }))
}
```

```{r}
# apply the model loop to the fake correlated data
allfirst.poolct <- rbindlist(lapply(2:4, experiment_loop, sim_min = 0.1, sim_max = 0.2, ct_dat = ct_fake_input))

group_by(allfirst.poolct, pool, prevalence, above.llod, limit) %>% 
  summarize(pos1=weighted.mean(pos, w=probability),
            pos_null=weighted.mean(pos, w=probability_null),
            total.tests=weighted.mean(tests, w=probability), 
            total.tests_null=weighted.mean(tests, w=probability_null), 
            tests.per.sample=weighted.mean(tests, w=probability)/mean(pool), # calculate tests per sample
            tests.per.sample_null=weighted.mean(tests, w=probability_null)/mean(pool),
            tn1=weighted.mean(tn, w=probability), 
            tp1=weighted.mean(tp, w=probability), 
            fn1=weighted.mean(fn, w=probability), 
            fp1=weighted.mean(fp, w=probability),
            ppa = weighted.mean(tp/(fn + tp), w=probability, na.rm = TRUE),
            ppa_null = weighted.mean(tp/(fn + tp), w=probability_null, na.rm = TRUE),
            tn_null=weighted.mean(tn, w=probability_null), 
            tp_null=weighted.mean(tp, w=probability_null), 
            fn_null=weighted.mean(fn, w=probability_null), 
            fp_null=weighted.mean(fp, w=probability_null)) -> apw
# rm(allfirst.poolct)

all <- as.data.frame(apw)
# rm(apw)

#all2 <- all %>% mutate(ppa=tp/(tp+fn))
#cpi <- as_tibble(BinomCI(all2$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
#allci <- bind_cols(all2,cpi)

cpi <- as_tibble(BinomCI(all$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
allci <- bind_cols(all,cpi)
```

Plots for Rho = (0.1, 0.2)
```{r}
#allci <- read.csv("test.csv")


#pdf("plot.ppa.corrindiv.real.pdf",width=8, height=5)  
ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence), linetype = "Correlated")) +
  # y= tp1/(tp1 + fn1)
  geom_line(allci, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence), linetype = "Independent")) +
  # y=tp_null/(tp_null + fn_null)
  facet_wrap(vars(factor(allci$above.llod, labels=c("5% samples Ct>LoD"))))+ #,"10% samples Ct>LoD","15% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Correlated Individuals (rho = 0.1 - 0.2) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
#dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
#pdf("plot.eff.corrindiv.real.pdf",width=8, height=5)  
ggplot() +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence),  linetype = "Correlated")) +
  geom_line(allci, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = "Independent")) +
  facet_wrap(vars(factor(allci$above.llod)))+#, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  ggtitle("Correlated Individuals (rho = 0.1 - 0.2) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.6,0),breaks=seq(0,1.6,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
#dev.off()

```



## Realistic scenario (test)
```{r correlated_individuals}
source("MCMC.R")
# pool.max = 20
pool.max = 20
experiment_loop <- function(above, ct_dat){
  ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45)) 
  threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
  
  rbindlist(lapply(1:pool.max, function (p) { # pool size
    # number of correlated individuals, number of uncorrelated individuals
    nb_group <- trunc(.5*p)
    nb_out <- p - nb_group
    # create correlation matrix (function of number of correlated individuals)
    Sigma <- matrix(runif(p^2, 0.1, 0.2), nrow = p, ncol = p) # generate random values from .1 to 0.9
    Sigma = 0.5 * (Sigma + t(Sigma)) # make matrix symmetrical
    diag(Sigma)= rep(1,p) # 1's on diagonal
    
    rbindlist(lapply(c(0.001,0.01,0.05,0.1, 0.15), function(prevalence) { # prevalence ("proportion positive tests")
      prev <- runif(p, 0, (2*prevalence))
      # prev <- gamma(alpha = prevalence, beta= 1)
      sim_probs_real = simulate_infections(nb_group, nb_out, Sigma, prev, B = 3000)
      
      rbindlist(lapply(0:p, function(positives) { # number of positives
        
        if (positives == 0) {
          data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                     above.llod = above.lod[above], 
                     concentration=0, 
                     probability_null = dbinom(positives, p, prevalence),
                     probability_real = sim_probs_real[which(sim_probs_real$n == 0), "Freq"], 
                     random=0, z.index=0,
                     call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
        } 
        else {
          dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives) 
          each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,rnorm(mean=0,sd=1.1,n=ncol(dat))) 
          data.frame(
            limit=lod,
            pool=p,
            pos=positives,
            prevalence=prevalence,
            above.llod=above.lod[above],
            concentration=each.conc,
            probability_null = dbinom(positives, p, prevalence)/n,
            probability_real= sim_probs_real[which(sim_probs_real$n == positives), "Freq"],
            random=sample(n,n,replace = TRUE)/n, #
            z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
                           sample(1:length(z_scores),n,replace=T))) %>%
            mutate(
              call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
              tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
              tn=0,
              tp=1 * (call.each.conc),
              fn=1 * (!call.each.conc),
              fp=0)
        }
      }))
    }))
  }))
}
```

```{r}
# apply the model loop to the fake correlated data
# allfirst.poolct <- rbindlist(lapply(1:length(above.lod), experiment_loop, ct_dat = ct_fake_input))
allfirst.poolct <- rbindlist(lapply(1:3, experiment_loop, ct_dat = ct_fake_input))

group_by(allfirst.poolct, pool, prevalence, above.llod, limit) %>% 
  summarize(pos_real=weighted.mean(pos, w=probability_real),
            pos_null=weighted.mean(pos, w=probability_null),
            total.tests=weighted.mean(tests, w=probability_real), 
            total.tests_null=weighted.mean(tests, w=probability_null), 
            tests.per.sample=weighted.mean(tests, w=probability_real)/mean(pool), # calculate tests per sample
            tests.per.sample_null=weighted.mean(tests, w=probability_null)/mean(pool),
            tn_real=weighted.mean(tn, w=probability_real), 
            tp_real=weighted.mean(tp, w=probability_real), 
            fn_real=weighted.mean(fn, w=probability_real), 
            fp_real=weighted.mean(fp, w=probability_real),
            ppa = weighted.mean(tp/(fn + tp), w=probability_real, na.rm = TRUE),
            ppa_null = weighted.mean(tp/(fn + tp), w=probability_null, na.rm = TRUE),
            tn_null=weighted.mean(tn, w=probability_null), 
            tp_null=weighted.mean(tp, w=probability_null), 
            fn_null=weighted.mean(fn, w=probability_null), 
            fp_null=weighted.mean(fp, w=probability_null)) -> apw
# rm(allfirst.poolct)

all <- as.data.frame(apw)
# rm(apw)

#all2 <- all %>% mutate(ppa=tp/(tp+fn))
#cpi <- as_tibble(BinomCI(all2$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
#allci <- bind_cols(all2,cpi)

cpi <- as_tibble(BinomCI(all$ppa*n,n,conf.level=0.95,method="clopper-pearson"))
allci <- bind_cols(all,cpi)

# write.csv(allci,"correlated_individuals_rho.1.2.csv")
write.csv(allci,"correlated_indiv_realistic_.1.2.csv")

#correlated_individuals_rho.1.2, calc_probs(p, prevalence, min=0.1, max=0.2, B=1000)
```

```{r}
# quick check: should have same results for null and alternative when pool size is 1
all %>%
  filter(pool == 1) %>%
  dplyr::select(ppa, ppa_null)
```

Plots for Rho = (0.1, 0.2)
```{r}
corr_indiv_.1.2_real <- read.csv("correlated_indiv_realistic_.1.2.csv")
corr_indiv_.1.2_null <- corr_indiv_.1.2_real
corr_indiv_.1.2_real$null <- "Correlated"
corr_indiv_.1.2_null$null <- "Uncorrelated"

pdf("plot.ppa.corrindiv.real.pdf",width=8, height=5)  
ggplot() +
  geom_line(corr_indiv_.1.2_real, mapping = aes(x=pool, y= ppa, color=as.factor(prevalence), linetype = null)) +
  # y= tp1/(tp1 + fn1)
  geom_line(corr_indiv_.1.2_null, mapping = aes(x=pool, y=ppa_null, color=as.factor(prevalence), linetype = null)) +
  # y=tp_null/(tp_null + fn_null)
  facet_wrap(vars(factor(corr_indiv_.1.2_real$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Correlated Individuals (rho = 0.1 - 0.2) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.corrindiv.real.pdf",width=8, height=5)  
ggplot() +
  geom_line(corr_indiv_.1.2_real, mapping = aes(x=pool, y= tests.per.sample, color=as.factor(prevalence), linetype = null)) +
  geom_line(corr_indiv_.1.2_null, mapping = aes(x=pool, y= tests.per.sample_null, color=as.factor(prevalence), linetype = null)) +
  facet_wrap(vars(factor(corr_indiv_.1.2_real$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") +
  ggtitle("Correlated Individuals (rho = 0.1 - 0.2) vs. Uncorrelated") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.6,0),breaks=seq(0,1.6,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

```

## Plots 

```{r}
allci_uncorr <- read.csv("model_output.csv")
allci_corr_indiv_12 <- read.csv("model_output_corr_indiv_.1.2.csv")
test <- read.csv("test.csv")
```

### Individual Plots
```{r generate_plots, eval=FALSE}
# Generate individual plots

# Print plot of pool size vs. PPA, panel grid of % samples with Ct > LoD, color by proportion test pos
pdf("plot.ppa.above.colprev.pdf",width=8, height=5)
ggplot(allci, aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence))) + 
  geom_line() +
  facet_wrap(vars(factor(above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive"))
dev.off()

# Print plot of pool size vs. PPA, panel grid of proportion test pos, color by % samples with Ct > LoD
pdf("plot.ppa.prev.colabove.pdf",width=8, height=5)
ggplot(allci, aes(x=pool, y=tp/(tp + fn), color=as.factor(above.llod))) + 
  geom_line() +
  facet_wrap(vars(factor(prevalence, labels=c("0.1% Tests positive", "1% Tests positive", "3% Tests positive", "5% Tests positive", "10% Tests positive","15% Tests positive")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion\nsamples\nCt > LoD"))
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.above.colprev.pdf",width=8, height=5)
ggplot(allci, aes(x=pool, y=tests.per.sample,color=as.factor(prevalence))) + 
  geom_line() +
  facet_wrap(vars(factor(above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") + 
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(trans="reverse", limits=c(1.2,0),breaks=seq(0,1.2,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive"))
dev.off()

```


### Plots comparing different sampling methods
#### Correlated INDIVIDUALS vs. uncorrelated
```{r}
# correlated (sampled w. grouping) vs. uncorrelated

#subset uncorrelated data to contain same obs as correlated data
# correlated data doesn't have pool size 1
allci_uncorr_sub <- allci_uncorr[allci_uncorr$X %in% allci_corr_indiv$X,]
allci_uncorr_sub$corr_type <- "Uncorrelated"
allci_corr_indiv$corr_type <- "Correlated"

# Print plot of pool size vs. PPA, panel grid of % samples with Ct > LoD, color by proportion test pos
pdf("plot.ppa.above.colprev.corr_indiv.pdf",width=8, height=5)
ggplot() +
  geom_line(allci_corr_indiv, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence), linetype = corr_type)) +
  geom_line(allci_uncorr_sub, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(prevalence), linetype = corr_type)) +
  facet_wrap(vars(factor(allci_corr_indiv$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Correlated Individuals") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.75,1),breaks=seq(0.75,1,.1)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

# Print plot of pool size vs. PPA, panel grid of proportion test pos, color by % samples with Ct > LoD
pdf("plot.ppa.prev.colabove.groupfrac1.both.pdf",width=8, height=5)
ggplot() + 
  geom_line(allci_corr_group1, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(above.llod), linetype = corr_type)) +
  geom_line(allci_uncorr_sub, mapping = aes(x=pool, y=tp/(tp + fn), color=as.factor(above.llod), linetype = corr_type)) +
  facet_wrap(vars(factor(allci_corr_group1$prevalence, labels=c("0.1% Tests positive", "1% Tests positive", "3% Tests positive", "5% Tests positive", "10% Tests positive","15% Tests positive")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Expected positive percent agreement") +
  ggtitle("Uncorrelated Data & Correlated Data Sampled by Group (Fraction = 1)") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0.5,1),breaks=seq(0.5,1,.1)) +
  guides(color=guide_legend(title="Proportion\nsamples\nCt > LoD")) + 
  guides(linetype=guide_legend(title="Correlation Structure")) 
dev.off()

# Print plot of pool size vs. average tests/sample, panel grid of %Ct > LoD, color by proportion test pos
pdf("plot.eff.above.colprev.groupfrac1.both.pdf",width=8, height=5)
ggplot() + 
  geom_line(allci_corr_group1, mapping = aes(x=pool, y=tests.per.sample, color=as.factor(prevalence), linetype = corr_type)) +
  geom_line(allci_uncorr_sub, mapping = aes(x=pool, y=tests.per.sample, color=as.factor(prevalence), linetype = corr_type)) +
  facet_wrap(vars(factor(allci_corr_group1$above.llod, labels=c("5% samples Ct>LoD","10% samples Ct>LoD","15% samples Ct>LoD","20% samples Ct>LoD","25% samples Ct>LoD","30% samples Ct>LoD")))) +
  theme_bw() + 
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + ylab("Average tests per sample") + 
  ggtitle("Uncorrelated Data & Correlated Data Sampled by Group (Fraction = 1)") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + scale_y_continuous(limits=c(0, 1.6),breaks=seq(0,1.6,0.2)) +
  guides(color=guide_legend(title="Proportion of\ntests positive")) +
  guides(linetype=guide_legend(title="Correlation Structure"))
dev.off()
```




##  Now with much more variability =---  let's add the prevalence variability 


```{r}
pool.max = 20
experiment_loop <- function(above, ct_dat){
  ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]<45)) 
  threshold.ct <- c(sample(ct_set, n, replace=T)) # sample ct_set with replacement
  
  rbindlist(lapply(1:pool.max, function (p) { # pool size
    # number of correlated individuals, number of uncorrelated individuals
    nb_group <- trunc(.5*p)
    nb_out <- p - nb_group
    # create correlation matrix (function of number of correlated individuals)
    Sigma <- matrix(runif(p^2, 0.1, 0.2), nrow = p, ncol = p) # generate random values from .1 to 0.9
    Sigma = 0.5 * (Sigma + t(Sigma)) # make matrix symmetrical
    diag(Sigma)= rep(1,p) # 1's on diagonal
    
    rbindlist(lapply(c(0.001,0.01,0.05,0.1, 0.15), function(prevalence) { # prevalence ("proportion positive tests")
      # prev <- gamma(alpha = prevalence, beta= 1)
      sim_probs_real = apply(sapply(1:500,function(b){
         prev <- runif(p, 0, (2*prevalence))
        compute_probas_level_all_variations(N, prev, tau, tau_relative_var, prev_relative_var, B=10000)
      }), 1, mean)
                              
      
      rbindlist(lapply(0:p, function(positives) { # number of positives
        
        if (positives == 0) {
          data.frame(limit=lod, pool=p, pos=positives, prevalence=prevalence, 
                     above.llod = above.lod[above], 
                     concentration=0, 
                     probability_null = dbinom(positives, p, prevalence),
                     probability_real = sim_probs_real[which(sim_probs_real$n == 0), "Freq"], 
                     random=0, z.index=0,
                     call.each.conc=FALSE, tests=1, tn=1,tp=0,fn=0,fp=0)
        } 
        else {
          dat <- matrix(sample(threshold.ct, positives * n, replace=T), nrow=positives) 
          each.conc = -log2(colSums(2^-dat)/p)+ifelse(dilution.vary.index==1,0,rnorm(mean=0,sd=1.1,n=ncol(dat))) 
          data.frame(
            limit=lod,
            pool=p,
            pos=positives,
            prevalence=prevalence,
            above.llod=above.lod[above],
            concentration=each.conc,
            probability_null = dbinom(positives, p, prevalence)/n,
            probability_real= sim_probs_real[which(sim_probs_real$n == positives), "Freq"],
            random=sample(n,n,replace = TRUE)/n, #
            z.index=ifelse(probit.mode.index<4,probit.z.indices[probit.mode.index],
                           sample(1:length(z_scores),n,replace=T))) %>%
            mutate(
              call.each.conc=probit[1+(z.index-1)*571+each.conc*10-(lod-35.9)*10,2]>random,
              tests=1 + p * (call.each.conc), # number of tests done (number positive pools + pool size)
              tn=0,
              tp=1 * (call.each.conc),
              fn=1 * (!call.each.conc),
              fp=0)
        }
      }))
    }))
  }))
}
```





