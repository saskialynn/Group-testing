---
title: "Simulations"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(fitdistrplus)
library(DescTools)
library(data.table)
library(dplyr)
```

Modified code from Claire's compute_proba_laws.R

The probability of having $K$ total positives in a pool of size $n$ is the probability of $k$ being infected in the community (probability of infection equals prevalence) and $K-k$ being infected via network transmission (probability of infection equals $\tau$).
$$
P(\sum Y_i = K) = 
     \sum_{k=1}^K \left( {n \choose k} \pi^k (1-\pi)^{n-k} {n-k \choose K- k} (1-(1-\tau)^k)^{K-k} ((1-\tau)^k)^{(n-K)} \right)
$$

```{r}
# inputs for debugging/testing purposes
# N <- 10
# prev <- .1
# tau <- .2
# B <- 1000
```

```{r}
##############################
####  Compute proba laws (approximation) with whilst allowing for some variability
####  INPUT
####  --------------------------------------------
####  N, prev, tau         : Size of the group, baseline prevalence and transmissibility
####  tau_graph_effect     : function to sample variables taus (graph effect). NULL if no graph effect.
####  tau_subject_effect   : function to sample variables taus (edge effect). NULL if no edge effect.
####  prev_graph_effect    : function to sample variables prevs (graph effect). NULL if no graph effect.
####  prev_subject_effect  : function to sample variables prevs (subject effect). NULL if no edge effect.
####

# functions to compute probability of K positives in a pool of size N
compute_pd <- function(N, K, prev){ # community infections
  sapply(1:K, function(k){choose(N,k) * prev^k * (1-prev)^(N-k) })
}

compute_td <- function(N, K, tau){ # network transmission
  sapply(1:(K), function(k){choose(N-k,K-k) *((1-tau)^k)^(N-K) * (1-(1-tau)^k)^(K-k)})
}
# functions to sample prevalence and tau from prior distributions
prev_graph_effect <- function(x, B){
  a = rnorm(B, log(x/(1-x)), 1)
  return((1/(1+exp(-a))))
}

prev_subject_effect <- function(x, N){ 
  a = sapply(x, function(xx){rnorm(N,log(xx/(1-xx)), 1)})
  return((1/(1+exp(-a))))
}

tau_graph_effect <- function(x, B){ # sample tau from unif[.5*tau, 1.5*tau]
  return(runif(B, 0.5 *x, 1.5 * x ))
}

tau_subject_effect <- function(x, N){ # check this with Claire
  a = sapply(x, function(xx){runif(N, 0.5*xx, 1.5*xx)}) # change to sampling from pareto 
  return(a)
}
```

```{r}
proba_laws <- function(N, prev, tau, 
                       tau_graph_effect=NULL,
                       tau_subject_effect=NULL, 
                       prev_graph_effect=NULL,
                       prev_subject_effect=NULL,
                       B=1000){
  if (is.null(prev_graph_effect) == FALSE){ # sample prevalence from prior
    prevs = prev_graph_effect(prev, B) # sample prevalence B times
    dist_nothing <- sapply(prevs, function(prev){(1-prev)^N}) # nobody infected from community
    dist_prevs = lapply(prevs, function(prev){ # 1:K infected from comm
      c((1-prev)^N, sapply(1:N, function(K){ compute_pd(N,K, prev)
      }))})  ### list (B length) of N+1 items (for 0:N infected), each of length k
  }else{ # point value of prev (not sampled)
    prevs = rep(prev, B)
    dist_nothing <- sapply(prevs, function(prev){(1-prev)^N})
    dist_prevs = lapply(prevs, function(prev){
      c((1-prev)^N, sapply(1:N, function(K){compute_pd(N,K, prev)
      }))})   ### list (B length) of K items of length k
  }
  
  if (is.null(prev_subject_effect) == FALSE){
    #### the prevalence is small, so we use a Poisson approximation to the binomial
    ## large n, small pi -> lambda = n(pi)
    dist_prevs = sapply(prevs, function(prev){
      #### generate subject effects
      prev_s = prev_subject_effect(prev, N) # sample prev
      lambda=  sum(prevs)
      return(sapply(0:N, function(K){exp(-lambda) * lambda^K/factorial(K)})) #Poisson PMF
    })}
  
  
  if (is.null(tau_graph_effect) == FALSE){
    taus = tau_graph_effect(tau, B)
    dist_taus = lapply(taus, function(tau){
      sapply(1:N, function(K){ compute_td(N,K, tau) # compute for each tau and value of N
      })})  ### N x B
  }else{
    taus = rep(tau, B)
    dist_taus = lapply(taus, function(tau){
      sapply(1:N, function(K){ compute_td(N,K, tau) 
      })})
  }
  if (is.null(tau_subject_effect) == FALSE){
    taus_f <- function(b){tau_subject_effect(x=tau, b)}
    dist_taus = lapply(1:B, function(b){ #for B simulations
      sapply(1:N, function(K){ 
        #### generate subject effects
        sapply(1:(K), function(k){
          # taus_v = taus_f((N-K)*K) 
          if (k<K){
            choose(N-k, K-k) * exp(sum(log(1-taus_f((N-K)*k)))) * exp(sum(sapply(1:(K-k), function(j){log(1-exp(sum(log(1-taus_f(k)))))})))  
          }else{
            exp(sum(log(1-taus_f((N-K)*K))))
          }
          
        })
      })})
  }
  
  
  #### Now compute the dot product
  test = sapply(1:B,function(b){
    c(dist_prevs[[b]][[1]], sapply(1:N, function(k){
      sum(dist_prevs[[b]][[(k+1)]] * dist_taus[[b]][[k]])
    }))
  }) 
    # make sure sums to 1 (since we are approximating)
  # normalize if needed 
  for (j in 1:ncol(test)){
    if(sum(test[,j])!=1){
      test[,j] = test[,j]/sum(test[,j])
    }
    
  }
  return(test)
}
```

Recall, $N$= group size, $K$= number of total positives, $k$ = number positive from community
$K-k$ = number positive from network
$N-k$ = number not positive from community (either negative or positive from network)
if (k<K): 
$$ 
{N-k \choose K-k} \times e^{\sum(log(1- \tau_f((N-K)*k)))} \times e^{\sum(sapply(1:(K-k), function(j){log(1-exp(sum(log(1-taus_f(k)))))})))}  
$$
$e^{\sum(log(1- \tau_f((N-K)*k)))}$ is the probability of individuals not infected by network or community
second term is individuals infected by network 

`taus_f((N-K)*k)` returns $(N-K)*k$ sampled $\tau$ values. 

else{
            exp(sum(log(1-taus_f((N-K)*K))))
          }


# Load the real data and extract the distribution parameters
```{r}
# Get all tests
tests <- data.table::fread("alltests_1mar24jun_v1.csv")

# Keep only valid positive ct values for first tests
tests %>% filter(result == "positive", firsttest==TRUE,!is.na(cttarget)) %>% pull(cttarget) -> cts

cts2 <- 2^(-cts)
# Fit Weibull Distribution
fw <- fitdist(cts, "weibull")

fw_norm <- fitdist(cts, "norm")
```

```{r}
## GENERATE UN-CORRELATED Ct DATA
# Add variable of proportion Ct value >LoD to model, as surrogate for differences in viral load distribution in different populations

# Number of replicates to generate ecdf offset values for above.
set.seed(42)
x<-5000

# Create simulated vector of ct values with same shape as that from Weibull distribution fit to ct values from real first tests
# 5000 draws from weibull with shape and scale parameters drawn from real data
ct_fake_input <- rweibull(x,fw[[1]][1],fw[[1]][2]) 

###3 Experiment proba law

proba_sum <- function(N, B=5000){
  sapply(1:N, function(p){
  sapply(1:B, function(b){
    -log2( sum(2^(-rweibull(p,fw[[1]][1],fw[[1]][2])))/N) 
  })
  })
}
#### Exepriment: number of Ct


# Create matrix of desired input parameters
# Change above.lod to % samples with ct value > LoD to reflect actual population of interest. Changing "lod" itself has no effect on model output.
lod <- 35
above.lod <- seq(0.05,0.5,0.05)
translation_vector <- seq(-10,15,0.01)
mat<-matrix(ncol=3,nrow=length(translation_vector))

# Loop 1: fix LOD; for known shift, what percent of samples are above LoD (a)? 
for(v in 1:length(translation_vector)){
  i=1 
  fn<-ecdf(subset(pmax(5,ct_fake_input+translation_vector[v]),
                  (pmax(5,ct_fake_input+translation_vector[v]))<45)) ## selects adequate values within the sample
			a<-1-fn(lod) ## percent of cvalues in the distribution above the LoD 
			mat[(v-1)+i,1]<-lod
			mat[(v-1)+i,2]<-a
			mat[(v-1)+i,3]<-translation_vector[v]
}

# Shift ct values for each LoD and %above lod
mat2<-matrix(ncol=3,nrow=length(above.lod))

# Loop 2: fix LOD; for known a, how much should you shift? 
for(j in 1:length(above.lod)){
	tmp<-subset(mat,mat[,1]==lod)
	u<-tmp[which.min(abs(above.lod[j]-tmp[,2])),3]
	mat2[j,1]<-lod[i]
	mat2[j,2]<-above.lod[j]
	mat2[j,3]<-u
}
```

# Include the probit  coefficients
The probits scores are necessary to compute the viral load.
```{r}
# probit data input
probit_input <- read.csv("probit_zscores_cts_tissue_agnostic.csv")
probit_t<-subset(probit_input,probit_input$z_value<=1.96 & probit_input$z_value>=-1.96)
probit<-probit_input[,c(2,4,3)]
probit<-probit[order(probit$z_value,probit$ct_value),]
z_scores<-as.numeric(unlist(distinct(probit,z_value)))

# Number of replicates for model
set.seed(42)
n <- 10000

pool.max<-20
probit.mode<-c("base","dsa.lower","dsa.upper","psa") 
probit.mode.index<-1 # 1 = no variation, 2, = LLN, 3 = ULN, 4 = probabilistic
probit.z.indices<-c(488,1,length(z_scores)) # 488 is a z score of 0 (base case) in the z index vector
dilution.vary.index<-1 # 1 = no variation, 2 = probabilistic
uu = probit_input %>% filter(z_value  > 1.93 &  z_value  < 1.99 | z_value < -1.93 & z_value  > - 1.97  |  z_value  == 0 )  %>% dplyr::select(ct_value, z_value,probit_probability_detection)
uu["z_value"] = as.factor(uu$z_value)
uuu = pivot_wider(uu, names_from = c( "z_value"), values_from = "probit_probability_detection" )
colnames(uuu) = c("ct_value", "median", "q025", "q975")
ggplot(uuu)+
  geom_line(aes(x=ct_value, y = median)) +
  geom_ribbon(aes(x=ct_value, ymin = q025, ymax=q975), alpha=0.5)+
  theme_bw() + 
  theme(legend.text=element_text(size=16))+labs(colour="Average Probability", fill="95% CI Range", size=14) + 
  theme(text = element_text(size=25),
        axis.text.x = element_text(angle=0)) + 
  xlab("Ct value") + 
  ylab("Probability of Detection") 
```

```{r}
pool.size <- pool.max
ct_dat <- ct_fake_input
above <-  6# 15% > LoD
# pool.size = n
B=1000
# RV to simulate sensitivity 
sens_fxn <- function(N, above=6){
  u = data.frame(t(sapply(1:N, function(positives){
    set.seed(123)
    ct_set <- pmax(5,subset(ct_dat + mat2[above,3], ct_dat + mat2[above,3]< 45)) 
    # mean(sapply(ct_set, function(x){ uu$m[which(uu$ct_value == round(x))]}))
    # (1) generate additional set of C_t values with above = 5-30% of values above each LoD; subset to only valid Ct values (<45)
    # (2) ct_set is the parallel maximum of the C_t values from (1) vs. 5 (why 5??)

    dat <- matrix(sample(ct_set, positives * B, replace=T), nrow=positives) 
    # sample data uniformly at random 
    # n samples of positives, rearrange into a matrix of positive rows, n columns
    each.conc = data.frame("x"= round(-log2(colSums(2^-dat)/N),1), type=paste0("N=", positives))
    # sd of 1.1 reflects confidence interval for deviation from perfect log2 dilution in assays
    # calculation dilution based on number of positives (colSum) in total pool size (p)
    # z.index= probit.z.indices[probit.mode.index]
    # probit score: probability of false positive or false negative test
    p = merge(probit %>% filter(z_value==0), each.conc, by.x = "ct_value", by.y="x", all.y=TRUE, all.x=FALSE)
    ## pp = merge(probit %>% filter(z_value <1.96 & z_value>-1.96), each.conc, by.x = "ct_value", by.y="x", all.y=TRUE)
    return(c(mean(p$probit_probability_detection),
            median(p$probit_probability_detection),
            sd(p$probit_probability_detection),
            as.numeric(quantile(p$probit_probability_detection, 0.025)),
            as.numeric(quantile(p$probit_probability_detection, 0.975))
                      ))
    # returns probability that a test is positive given k positives in sample
  })))
  colnames(u) = c("mean", "median", "sd_mean", "q025", "q975")
  return(u)
} 


# prev_graph_effect <- function(x, B){
#   a = rnorm(B, log(x/(1-x)), 3)
#   return((1/(1+exp(-a))))
# }

it =1
# N <- pool.max
# N <- 10
# prev <- 0.1
# tau <- 0.3
res = data.frame("sensit" =c(0),
                      "ppa" = c(0),
                     "num_tests" =c(0),
                     "type" =c(0), 
                     "caught_cases" = c(0),
                    "caught_cases_persample" =c(0), 
          "pool_size" =c(0),  
          "prev" = c(0), 
          "tau"=c(0) )

names = c("Fixed", "Tau Graph Effect","Pi Graph Effect","All Graph Effect")
for (N in 1:pool.max){
  for (prev in c(0.005, 0.01, 0.05, 0.1, 0.15)){
  # 0.001, 0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.12, 0.15, 0.17, 0.2, 0.25, 0.3
  for(tau in seq(0, 0.6, by = 0.1)){
    # seq(from=0.00, to=0.7, by=0.005)
    print(c(prev,tau, N))
    a = list("Fixed" = proba_laws(N, prev, tau, 
                        tau_graph_effect=NULL,
                        tau_subject_effect=NULL, 
                        prev_graph_effect=NULL,
                        prev_subject_effect=NULL,
                        B=1000),
            "Tau Graph Effect" = proba_laws(N, prev, tau, 
                        tau_graph_effect=tau_graph_effect,
                        tau_subject_effect=NULL, 
                        prev_graph_effect=NULL,
                        prev_subject_effect=NULL,
                        B=1000),
             "Pi Graph Effect"= proba_laws(N, prev, tau, 
                        tau_graph_effect=NULL,
                        tau_subject_effect=NULL, 
                        prev_graph_effect=prev_graph_effect,
                        prev_subject_effect=NULL,
                        B=1000),
             "All Graph Effect"= proba_laws(N, prev, tau, 
                        tau_graph_effect=tau_graph_effect,
                        tau_subject_effect=NULL, 
                        prev_graph_effect=prev_graph_effect,
                        prev_subject_effect=NULL,
                        B=1000)
    )
    
    
    
    for (n in names){
      sens <- sens_fxn(N) # prob(test positive | sum(Y_i) = k)
      individual_sens  <- sens_fxn(1)
      prob_pos <- sapply(1:B, function(b){sum(a[[n]][2:(N+1), b])})
      prob_pos_test <- sapply(1:B, function(b){sum(a[[n]][2:(N+1), b] * unlist(sens$mean))})
      res = rbind(res, data.frame(
          "sensit" = sapply(1:B, function(b){
            sum(a[[n]][2:(N+1), b] * sens$mean)/sum(a[[n]][2:(N+1), b])}), #sensitivity
          
          "ppa" = sapply(1:B, function(b){
            (sum(a[[n]][2:(N+1), b] * sens$mean)/sum(a[[n]][2:(N+1), b]))/individual_sens$mean}),
          
          "num_tests" = (1/N) + prob_pos_test,
          
          "caught_cases" = sapply(1:B, function(b){
            (sum(c(1:N)*a[[n]][2:(N+1), b] * sens$mean)/sum(a[[n]][2:(N+1), b]))}),
          "caught_cases_persample" = sapply(1:B, function(b){ # missed cases per sample
            (sum(c(1:N)*a[[n]][2:(N+1), b] * (sens)))})/((1/N) + prob_pos),
          
          "type" =n, 
          "pool_size" = N, 
          "prev" = prev, 
          "tau" = tau)
      )
      it = it + 1
    }
  }
} 
}

```

```{r}
res %>%
  group_by(tau, prev, pool_size) %>%
  summarise(n())
```


```{r}
# calculate sd, mean and 2.5% and 97.5% quartiles for each outcome variable of interest
# grouped by type, pool_size, tau and prev
sds <- res %>% group_by(type, pool_size, tau, prev) %>% summarise_all(sd)%>% mutate(variable="sd")
means <-  res %>% group_by(type, pool_size, tau, prev) %>% summarise_all(mean) %>% mutate(variable="mean")
quantile_low <- res %>% group_by(type, pool_size, tau, prev) %>% summarise_all(quantile, 0.025)%>% mutate(variable="q025")
quantile_upper <- res %>% group_by(type, pool_size, tau, prev) %>% summarise_all(quantile, 0.975) %>% mutate(variable="q975")



conf_int <- rbind(sds, means,quantile_upper, quantile_low)

conf_int <-pivot_wider(conf_int, id_cols=c("type", "pool_size", "tau", "prev"), names_from="variable", values_from = c("sensit",
                                                                                                            "ppa", "num_tests",
                                                                                                            "caught_cases","caught_cases_persample"))
```


```{r}
tau_val <- seq(0, .6, .1)

ggplot(conf_int %>% filter(tau==0.1), aes(x=pool_size, y= num_tests_mean)) +
  geom_line(size=1) +
  facet_wrap(~ prev) +
  #geom_errorbar(aes(x=pool_size, ymin=ci_lower, ymax=ci_upper), width=.3) +
  geom_ribbon(aes(x=pool_size, ymin = num_tests_q025, ymax = num_tests_q975, fill =type), alpha=0.4)+
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  # scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + 
  ylab("Num tests") +
  #scale_y_continuous(limits=c(0.5, 1)) +
  ggtitle(paste("Tau =", 0.1)) +
  #scale_x_continuous(limits=c(1,10), breaks=seq(0,10,2)) +
  guides(color=guide_legend(title="Correlation"))  


library(ggpubr)

pdf("sensitivity_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(sens_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("sensitivity_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(sens_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
# use geom_ribbon() instead
# jitter the position so they don't overlap exactly
# compare to the null: binomial (non correlation, independence)
# to compute CI: empirical quantiles 97.5 and 2.5 from the simulation
# fix y-limits to be fro 0.5 to 1
# fix specific value of tau and prev
```

```{r}
ppa_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_ppa, color=as.factor(type))) +
    geom_line() +
    #geom_point()+
    facet_wrap(~ prev) +
    geom_ribbon(aes(x=pool_size, ymin = ppa_ql, ymax = ppa_qu, fill = as.factor(type)), show.legend = F, alpha = 0.3)+
    theme_bw() + 
    theme(axis.title.x = element_blank())+
    xlab("Pool size") + 
    ylab("PPA") +
    ggtitle(paste("Tau =", i)) +
    guides(color=guide_legend(title="Correlation")) 
})

pdf("ppa_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(ppa_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("ppa_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(ppa_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
```

```{r}
tests_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_tests, color=as.factor(type))) +
    geom_line() +
    #geom_point()+
    facet_wrap(~ prev) +
    geom_ribbon(aes(x=pool_size, ymin = num_tests_ql, ymax = num_tests_qu, fill = as.factor(type)), show.legend = F, alpha = 0.3)+
    theme_bw() + 
    theme(axis.title.x = element_blank())+
    xlab("Pool size") + 
    ylab("Expected Number Tests / Sample") +
    ggtitle(paste("Tau =", i)) +
    guides(color=guide_legend(title="Correlation")) 
})
pdf("tests_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(tests_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("tests_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(tests_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
```
```{r}
missed_plots <- lapply(tau_val, function(i){
  ggplot(conf_int[conf_int$tau == i,], aes(x=pool_size, y= mean_missed, color=as.factor(type))) +
    geom_line() +
    #geom_point()+
    facet_wrap(~ prev) +
    #geom_errorbar(aes(x=pool_size, ymin=ci_lower, ymax=ci_upper), width=.3) +
    geom_ribbon(aes(x=pool_size, ymin = missed_cases_ql, ymax = missed_cases_qu, fill = as.factor(type)), show.legend = F, alpha = 0.3)+
    theme_bw() + 
    theme(axis.title.x = element_blank())+
    # scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
    xlab("Pool size") + 
    ylab("Missed Cases / Sample") +
    ggtitle(paste("Tau =", i)) +
    guides(color=guide_legend(title="Correlation")) 
})
pdf("missed_figa.pdf",width=8, height=5)  
do.call(ggarrange, c(missed_plots[1:4], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()

pdf("missed_figb.pdf",width=8, height=5)  
do.call(ggarrange, c(missed_plots[5:7], ncol = 2, nrow = 2, common.legend=TRUE, legend = "bottom"))
dev.off()
```
```{r}
pdf("all_tau_3.pdf", width = 8, height = 5)
ggarrange(sens_plots[[4]], ppa_plots[[4]], tests_plots[[4]], missed_plots[[4]], 
          nrow = 2, ncol = 2, common.legend = TRUE, legend = "bottom")
dev.off()
pdf("all_tau_0.pdf", width = 8, height = 5)
ggarrange(sens_plots[[1]], ppa_plots[[1]], tests_plots[[1]], missed_plots[[1]], 
          nrow = 2, ncol = 2, common.legend = TRUE, legend = "bottom")
dev.off()
```

```{r}
# table to nummerically summarize some of the graphical results
null_res <- res %>%
  filter(tau == 0 & type == "Fixed") 
alt_res <- res %>%
  filter(tau == "0.3" & type == "All Graph Effect") 


summary_table <- allci %>%
  mutate(ppa_diff=ppa - ppa_null, 
         test_diff=tests.per.sample - tests.per.sample_null, 
         ppa_pct_increase = ((ppa-ppa_null)/ppa_null)*100, 
         test_pct_decrease = ((tests.per.sample - tests.per.sample_null)/tests.per.sample_null)*100) %>%
  filter(pool == 4 | pool == 18) %>%
  filter(prevalence == 0.001 | prevalence == 0.1) %>%
  dplyr::select(pool, prevalence, tau, 
                ppa_null, ppa_pct_increase,
                tests.per.sample_null, test_pct_decrease,
                ppa_diff, test_diff)

write.csv(summary_table, "summary_table_ppa.csv")
```



```{r}
ggplot() +
  geom_point(res, mapping = aes(x=pool_size, y= sensit, color=as.factor(type))) +
  facet_grid(tau ~ prev) +
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + 
  ylab("Sensitivity") +
  # ggtitle("Correlated vs. IID Samples") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + 
  guides(color=guide_legend(title="Correlation")) 

# use geom_ribbon()
```

```{r}
ggplot() +
  geom_point(res, mapping = aes(x=pool_size, y= num_tests, color=as.factor(type))) +
  facet_grid(tau ~ prev) +
  theme_bw() + 
  theme(axis.title.x = element_blank())+
  scale_color_brewer(palette="RdYlBu", name="Prevalence") + 
  xlab("Pool size") + 
  ylab("Expected Number Tests / Sample") +
  # ggtitle("Correlated vs. IID Samples") +
  scale_x_continuous(limits=c(1,20), breaks=seq(0,20,2)) + 
  guides(color=guide_legend(title="Correlation")) 
```



```{r}
res$
  ggplot(res) +
  
  
  
  test = sapply(1:B, function(b){
    sum(a[, b] * c(0,sens))
  })

test = sapply(1:B, function(b){
  sum(a[2:(N+1), b] * sens)/sum(a[2:(N+1), b])
})
#### Is this legit?  ## Need to run the simulations
```




